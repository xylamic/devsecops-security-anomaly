{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Tuple, Dict\n",
    "module_path = os.path.abspath(os.path.join('../library/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import analysis_utils\n",
    "from anomalous_user import AnomalousUser\n",
    "\n",
    "turn_leavers_into_binary = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "This notebook is used to aggregate all the feature sets into a dataset that's ready for training. The model is then training and tested against a specified dataset to capture results/anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from datetime import datetime, timezone\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "input_file = '../data/v3-ml-complete-features-20250426.csv'\n",
    "users_file = '../data/allorgs_useremail_20240510.csv'\n",
    "\n",
    "anomaly_ranked_results_html = \"../data/anomalies-ranked-20250426.html\"\n",
    "anomaly_ranked_results_md = \"../data/anomalies-ranked-20250426.md\"\n",
    "\n",
    "run_as_global_only = False # Determines whether to exclude UEBA\n",
    "\n",
    "# read in the dataset\n",
    "df_features = pd.read_csv(input_file)\n",
    "\n",
    "# convert 'day' column to a datetime\n",
    "df_features['day'] = pd.to_datetime(df_features['day'])\n",
    "\n",
    "if run_as_global_only:\n",
    "    # drop all columsn that start with 'zscore_'\n",
    "    df_features = df_features.loc[:, ~df_features.columns.str.startswith('zscore_')]\n",
    "\n",
    "df_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any duplicate rows\n",
    "print(f\"Number of rows before removing duplicates: {len(df_features)}\")\n",
    "df_features = df_features.drop_duplicates()\n",
    "print(f\"Number of rows after removing duplicates: {len(df_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the range of the 'day' column\n",
    "min_day = df_features['day'].min()\n",
    "max_day = df_features['day'].max()\n",
    "print(f'Minimum day: {min_day}')\n",
    "print(f'Maximum day: {max_day}')\n",
    "\n",
    "# get the number of days in the dataset\n",
    "num_days = (max_day - min_day).days + 1\n",
    "print(f'Number of days in the dataset: {num_days}')\n",
    "\n",
    "target_file = f'../data/results-{datetime.strftime(max_day, \"%Y%d\")}.csv'\n",
    "print(f\"Target file: {target_file}\")\n",
    "\n",
    "# get the count of rows for each 'day'\n",
    "print(\"Number of items per day:\")\n",
    "df_features['day'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting at the max day, check the number of rows for each of the past number of days\n",
    "for i in range(0, num_days):\n",
    "    day_to_test = max_day - pd.DateOffset(days=i)\n",
    "    print(f\"Day to test: {day_to_test}\")\n",
    "    print(f\"Day shape: {df_features[df_features['day'] == day_to_test].shape}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "### Test the contamination value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination_test_value = 0.1\n",
    "\n",
    "# drop the actor and the unique_ips (replaced by in/out subnet)\n",
    "df_input = df_features.drop(columns=['actor', 'day'])\n",
    "\n",
    "# create the isolation forest model\n",
    "clf = IsolationForest(random_state=0, contamination=contamination_test_value)\n",
    "\n",
    "# fit the model to the dataset\n",
    "clf.fit(df_input)\n",
    "\n",
    "df_anomaly_values = clf.decision_function(df_input)\n",
    "print(f\"Anomaly values: {len(df_anomaly_values)}\")\n",
    "\n",
    "# copy df_features and add the anomaly values\n",
    "df_c01results = df_features.copy()\n",
    "df_c01results['anomaly_score'] = df_anomaly_values\n",
    "\n",
    "df_c01results = df_c01results.sort_values(by='day')\n",
    "\n",
    "# get the smallest anomaly score\n",
    "min_anomaly_score = df_c01results['anomaly_score'].min()\n",
    "\n",
    "# Assuming df_results contains the anomaly_score and day columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_c01results['anomaly_score'], df_c01results['day'], alpha=0.5)\n",
    "plt.title('Scatter Plot of Anomaly Score vs Day')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Day')\n",
    "plt.grid(True)\n",
    "\n",
    "anomaly_score_threshold: float = -0.38 if run_as_global_only else -0.32\n",
    "\n",
    "# add a vertical line\n",
    "plt.axvline(x=anomaly_score_threshold, color='r', linestyle='--', label='Threshold')\n",
    "\n",
    "# add a light red box from threshold to end\n",
    "plt.axvspan(anomaly_score_threshold, min_anomaly_score - 0.01, color='red', alpha=0.1)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "# get the MAD-z scores\n",
    "df_c01results['mad_z'] = (df_c01results['anomaly_score'] - df_c01results['anomaly_score'].mean()) / df_c01results['anomaly_score'].std()\n",
    "df_c01results['mad_z'] = df_c01results['mad_z'].abs()\n",
    "df_c01results['mad_z'] = df_c01results['mad_z'].round(2)\n",
    "\n",
    "# get the mad_z threshold that results in 26 anomalies\n",
    "mad_z_threshold = 0.0\n",
    "while len(df_c01results[df_c01results['mad_z'] > mad_z_threshold]) > 26:\n",
    "    mad_z_threshold += 0.01\n",
    "print(f\"mad_z threshold: {mad_z_threshold}\")\n",
    "print(f\"Number of anomalies with mad_z > {mad_z_threshold}: {len(df_c01results[df_c01results['mad_z'] > mad_z_threshold])}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination_test_value = 0.1\n",
    "\n",
    "# split into 3 data sets\n",
    "df_input_1 = df_features[df_features['day'] < pd.Timestamp('2025-01-01', tz=timezone.utc)]\n",
    "df_input_2 = df_features[(df_features['day'] >= '2025-01-01') & (df_features['day'] < '2025-02-01')]\n",
    "df_input_3 = df_features[(df_features['day'] >= '2025-02-01') & (df_features['day'] < '2025-03-01')]\n",
    "df_input_4 = df_features[(df_features['day'] >= '2025-03-01') & (df_features['day'] < '2025-04-01')]\n",
    "df_input_5 = df_features[df_features['day'] >= '2025-04-01']\n",
    "\n",
    "# iterate for each of the 3 data sets\n",
    "for i, df_set in enumerate([df_input_1, df_input_2, df_input_3, df_input_4, df_input_5]):\n",
    "    df_input = df_set.drop(columns=['actor', 'day'])\n",
    "\n",
    "    print(f\"Data set {i + 1}\")\n",
    "    print(f\"Shape: {df_input.shape}\")\n",
    "\n",
    "    # create the isolation forest model\n",
    "    clf = IsolationForest(random_state=0, contamination=contamination_test_value)\n",
    "\n",
    "    # fit the model to the dataset\n",
    "    clf.fit(df_input)\n",
    "\n",
    "    df_anomaly_values = clf.decision_function(df_input)\n",
    "    print(f\"Anomaly values: {len(df_anomaly_values)}\")\n",
    "\n",
    "    # copy df_features and add the anomaly values\n",
    "    df_c01results = df_set.copy()\n",
    "    df_c01results['anomaly_score'] = df_anomaly_values\n",
    "\n",
    "    df_c01results = df_c01results.sort_values(by='day')\n",
    "\n",
    "    # get the smallest anomaly score\n",
    "    min_anomaly_score = df_c01results['anomaly_score'].min()\n",
    "\n",
    "    # Assuming df_results contains the anomaly_score and day columns\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_c01results['anomaly_score'], df_c01results['day'], alpha=0.5)\n",
    "    plt.title('Scatter Plot of Anomaly Score vs Day')\n",
    "    plt.xlabel('Anomaly Score')\n",
    "    plt.ylabel('Day')\n",
    "    plt.grid(True)\n",
    "\n",
    "    anomaly_score_threshold: float = -0.38 if run_as_global_only else -0.32\n",
    "\n",
    "    # add a vertical line\n",
    "    plt.axvline(x=anomaly_score_threshold, color='r', linestyle='--', label='Threshold')\n",
    "\n",
    "    # add a light red box from threshold to end\n",
    "    plt.axvspan(anomaly_score_threshold, min_anomaly_score - 0.01, color='red', alpha=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the contaimination value based on the chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of anomalies with score < threshold\n",
    "anomalies_below_threshold = df_c01results[df_c01results['anomaly_score'] <= anomaly_score_threshold]\n",
    "num_anomalies_below_threshold: int = len(anomalies_below_threshold)\n",
    "num_anomalies: int = len(df_c01results)\n",
    "num_total_rows: int = len(df_features)\n",
    "perc_anomalies_below_threshold: float = float(num_anomalies_below_threshold) / float(num_anomalies)\n",
    "contamination_value: float = num_anomalies_below_threshold / num_total_rows\n",
    "\n",
    "print(f\"Number of anomalies with score < {anomaly_score_threshold}: {num_anomalies_below_threshold} of {num_anomalies}\")\n",
    "print(f\"Percentage of anomalies with score < {anomaly_score_threshold}: {perc_anomalies_below_threshold * 100:.2f}%\")\n",
    "print(f\"Total number of rows: {num_anomalies * (1/contamination_test_value)}\")\n",
    "print(f\"Contamination value calculated: {contamination_value:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the actor and the unique_ips (replaced by in/out subnet)\n",
    "df_input = df_features.drop(columns=['actor', 'day'])\n",
    "\n",
    "# create the isolation forest model\n",
    "clf = IsolationForest(random_state=0, contamination=contamination_value)\n",
    "\n",
    "# fit the model to the dataset\n",
    "clf.fit(df_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_email(df, users_file):\n",
    "    # open the export-sede-x-1707016946.csv file and read all lines\n",
    "    with open(users_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # for each line, split into a list of values\n",
    "    lines = [line.split(',') for line in lines]\n",
    "\n",
    "    # for each line, make the first value a key and the last value a value in a dictionary\n",
    "    lines = {line[0]: line[-2] for line in lines}\n",
    "\n",
    "    # for each actor in df_anomly, add the email address for the actor which is the value in the dictionary\n",
    "    df['email'] = df['actor'].map(lines)\n",
    "\n",
    "\n",
    "def test_dataset(df, clf, day_to_test) -> list[tuple[str, datetime, float, str]]:\n",
    "\n",
    "    df_test = df[df['day'] == day_to_test].copy()\n",
    "    df_test_results = df_test.copy(deep=False)\n",
    "\n",
    "    df_test = df_test.drop(columns=['actor', 'day'])\n",
    "\n",
    "    # get the anomaly scores for each row in the dataset\n",
    "    df_test_results[\"anomaly_score\"] = clf.decision_function(df_test)\n",
    "    df_test_results[\"anomaly\"] = clf.predict(df_test)\n",
    "\n",
    "    # filter the dataset where anomaly is -1 and sort by anomaly scores\n",
    "    df_anomaly = df_test_results[df_test_results['anomaly'] == -1].sort_values(by='anomaly_score', ascending=True)\n",
    "\n",
    "    append_email(df_anomaly, users_file)\n",
    "\n",
    "    columns_to_move = ['anomaly_score', 'anomaly', 'email']\n",
    "\n",
    "    columns = list(df_anomaly.columns)\n",
    "    for col in columns_to_move:\n",
    "        columns.remove(col)\n",
    "\n",
    "    columns.insert(2, columns_to_move[0])\n",
    "    columns.insert(3, columns_to_move[1])\n",
    "    columns.insert(4, columns_to_move[2])\n",
    "\n",
    "    df_anomaly = df_anomaly[columns]\n",
    "    df_anomaly.drop(columns=['anomaly'], inplace=True)\n",
    "\n",
    "    anomalies = []\n",
    "    for index, row in df_anomaly.iterrows():\n",
    "        anomalies.append((row['actor'], row['day'], row['anomaly_score'], row['email']))\n",
    "\n",
    "    return anomalies\n",
    "\n",
    "\n",
    "# for each of the last number of days days, test the dataset\n",
    "df_anomalies = []\n",
    "for i in range(0, num_days):\n",
    "    day_to_test = max_day - pd.DateOffset(days=i)\n",
    "    print(f\"Testing day: {day_to_test}...\")\n",
    "    df_anomalies_iter = test_dataset(df_features, clf, day_to_test)\n",
    "    df_anomalies.extend(df_anomalies_iter)\n",
    "\n",
    "df_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the df_features\n",
    "df_results = df_features.copy()\n",
    "\n",
    "# add a column to the df_results for the anomaly score\n",
    "df_results['anomaly_score'] = np.nan\n",
    "\n",
    "# add a column to the df_results for the email\n",
    "df_results['email'] = np.nan\n",
    "\n",
    "# for each anomaly in df_anomalies, set the anomaly score in df_results\n",
    "for anomaly in df_anomalies:\n",
    "    actor, day, anomaly_score, email = anomaly\n",
    "    df_results.loc[(df_results['actor'] == actor) & (df_results['day'] == day), 'anomaly_score'] = anomaly_score\n",
    "    # df_results.loc[(df_results['actor'] == actor) & (df_results['day'] == day), 'email'] = email\n",
    "\n",
    "# remove all rows where the anomaly score is NaN\n",
    "df_results = df_results[~df_results['anomaly_score'].isnull()]\n",
    "\n",
    "# sort the dataset by day and anomaly score\n",
    "df_results = df_results.sort_values(by=['anomaly_score', 'day'], ascending=[True, False])\n",
    "\n",
    "# display the results with only actor, day, anomaly score, and email\n",
    "df_results[['actor', 'day', 'anomaly_score', 'email']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump anomalies detected to a csv file\n",
    "df_results.to_csv(target_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 95%/99% for each column in df_features\n",
    "df_stat_cmp = df_results.describe(percentiles=[0.95, 0.99]).transpose()\n",
    "\n",
    "# remove all columns that are not in the 95% value\n",
    "df_stat_cmp = df_stat_cmp.drop(columns=['count', 'mean', 'std', 'min', 'max', '50%'])\n",
    "\n",
    "# remove any column that has 'zscore' in the name\n",
    "df_stat_cmp = df_stat_cmp[~df_stat_cmp.index.str.contains('zscore')]\n",
    "\n",
    "df_stat_cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ausers:List[AnomalousUser] = list()\n",
    "\n",
    "# for each row in df_results\n",
    "for index, row in df_results.iterrows():\n",
    "\n",
    "    auser = AnomalousUser(row['actor'], row['day'], row['anomaly_score'], row['email'])\n",
    "    ausers.append(auser)\n",
    "    \n",
    "    # for every other column in the row, print the non-zero values\n",
    "    for col in df_results.columns:\n",
    "        if col in ['actor', 'day', 'anomaly_score', 'email']:\n",
    "            continue\n",
    "\n",
    "        zscore_col = \"zscore\" in col\n",
    "        if zscore_col:\n",
    "            if row[col] >= 3.0:\n",
    "                auser.add_anomaly(col, row[col], AnomalousUser.AnomalyType.ZSCORE, 3.0)\n",
    "        else:\n",
    "            if col in ['leaver_action', 'is_weekend'] and row[col] > 0:\n",
    "                auser.add_anomaly(col, row[col], AnomalousUser.AnomalyType.FIXED, 1)\n",
    "            elif row[col] >= df_stat_cmp.loc[col].values[1] and row[col] > 0:\n",
    "                auser.add_anomaly(col, row[col], AnomalousUser.AnomalyType.NINETYNINE, df_stat_cmp.loc[col].values[1])\n",
    "            elif row[col] >= df_stat_cmp.loc[col].values[0] and row[col] > 0:\n",
    "                auser.add_anomaly(col, row[col], AnomalousUser.AnomalyType.NINETYFIVE, df_stat_cmp.loc[col].values[0])\n",
    "\n",
    "# group ausers by 'day'\n",
    "ausers_groups: Dict[str, List[AnomalousUser]] = dict()\n",
    "for auser in ausers:\n",
    "    if auser.day in ausers_groups:\n",
    "        ausers_groups[auser.day].append(auser)\n",
    "    else:\n",
    "        ausers_groups[auser.day] = [auser]\n",
    "\n",
    "# for sorted each day, print the number of anomalies\n",
    "for day, ausers2 in sorted(ausers_groups.items(), key=lambda x: x[0]):\n",
    "    print(f\"Day: {day} - Number of anomalies: {len(ausers2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine most anomalous features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutoff analysis logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# cutoff test values\n",
    "cutoff_test = [95.0, 96.0, 97.0, 98.0, 99.0, 99.1, 99.2, 99.3, 99.4, 99.5, 99.6, 99.7, 99.8, 99.9, 99.99]\n",
    "\n",
    "# create a list of 0s for the length of the cutoff_test\n",
    "cutoff_values = [0] * len(cutoff_test)\n",
    "cutoff_mins = [1000] * len(cutoff_test)\n",
    "\n",
    "ranked_results = []\n",
    "\n",
    "# for each row in df_results\n",
    "for index, row in df_results.iterrows():\n",
    "\n",
    "    feature_ranks = list[tuple[str, float, any]]()\n",
    "\n",
    "    # for each feature in the row, determine the percentile of the feature compared to the df_features\n",
    "    for col in df_results.columns:\n",
    "        if col in ['actor', 'day', 'anomaly_score', 'email']:\n",
    "            continue\n",
    "        \n",
    "        # Calculate the percentile rank of the given value\n",
    "        percentile_rank = stats.percentileofscore(df_features[col], row[col])\n",
    "\n",
    "        feature_ranks.append((col, percentile_rank, row[col]))\n",
    "    \n",
    "    # sort the feature ranks by the percentile rank\n",
    "    feature_ranks = sorted(feature_ranks, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # reorder the feature ranks to put those with 'zscore' in the name at the end\n",
    "    feature_ranks = sorted(feature_ranks, key=lambda x: 'zscore' in x[0])\n",
    "\n",
    "    ranked_results.append((row['actor'], row['day'], row['anomaly_score'], row['email'], feature_ranks))\n",
    "\n",
    "    for i, cutoff in enumerate(cutoff_test):\n",
    "        cutoff_counts = 0\n",
    "        # for each feature, if the percentile rank is greater than the cutoff, increment the cutoff_values\n",
    "        for feature in feature_ranks:\n",
    "            if feature[1] >= cutoff:\n",
    "                cutoff_counts += 1\n",
    "        cutoff_values[i] += cutoff_counts\n",
    "\n",
    "        if cutoff_counts < cutoff_mins[i]:\n",
    "            cutoff_mins[i] = cutoff_counts\n",
    "\n",
    "# divide the cutoff_values by the number of rows in df_results to get the percentage\n",
    "cutoff_values = [val / df_results.shape[0] for val in cutoff_values]\n",
    "\n",
    "# print the cutoff_values for each cutoff_test\n",
    "for i, cutoff in enumerate(cutoff_test):\n",
    "    print(f\"{cutoff} cutoff_avg: {cutoff_values[i]: 0.2f}, cutoff_min: {cutoff_mins[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutoff results\n",
    "\n",
    "Cutoff analysis was performed on each of the 180d to determine the optimal cutoff to minimize the results to be investigated once anoamalies have been identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a markdown file that shows the list of anomalies, grouped by the user in order of anomaly score (lowest to higest).\n",
    "\"\"\"\n",
    "\n",
    "cutoff_key = 99.0\n",
    "\n",
    "def create_markdown_file(ranked_results, cutoff_key, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f\"# Anomalous Users: {min_day.strftime('%Y-%m-%d')} to {max_day.strftime('%Y-%m-%d')}\\n\")\n",
    "\n",
    "        num_unique_actors = len(set([result[0] for result in ranked_results]))\n",
    "        f.write(f\"This is {(max_day - min_day).days} days with {num_unique_actors} actor performing {len(ranked_results)} anomalies.\\n\")\n",
    "\n",
    "        # group the results by actor\n",
    "        ranked_results = sorted(ranked_results, key=lambda x: (x[0], x[2]), reverse=False)\n",
    "\n",
    "        last_actor: str = \"\"\n",
    "        for result in ranked_results:\n",
    "            actor, day, anomaly_score, email, feature_ranks = result\n",
    "            actor_str: str = \"\"\n",
    "            new_anomaly: bool = True\n",
    "            if actor != last_actor:\n",
    "                last_actor = actor\n",
    "\n",
    "                f.write(\"\\n\")\n",
    "                f.write(f\"## {actor}, {email}\\n\")\n",
    "                f.write(\"| Day | Anomaly Score | Feature | % Rank | Value |\\n\")\n",
    "                f.write(\"|-----|---------------|---------|--------|-------|\\n\")                \n",
    "            \n",
    "            for feature in feature_ranks:\n",
    "                col, percentile_rank, value = feature\n",
    "                val_str = str(value)\n",
    "                if '.' in val_str:\n",
    "                    val_str: str = val_str[:val_str.find('.') + 3]\n",
    "\n",
    "                if percentile_rank >= cutoff_key or (col in ['is_weekend'] and value > 0):\n",
    "                    if new_anomaly:\n",
    "                        f.write(f\"| {day} | {anomaly_score:0.6f} | {col} | {percentile_rank:.2f} | {val_str} |\\n\")\n",
    "                        new_anomaly = False\n",
    "                    else:\n",
    "                        f.write(f\"| | | {col} | {percentile_rank:.2f} | {val_str} |\\n\")\n",
    "\n",
    "create_markdown_file(ranked_results, cutoff_key, anomaly_ranked_results_md)\n",
    "\n",
    "# print the file as markdown in the cell\n",
    "with open(anomaly_ranked_results_md, 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "# convert the markdown file to html\n",
    "os.system(f\"pandoc {anomaly_ranked_results_md} -o {anomaly_ranked_results_html}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of each feature and the number of times it appears in the anomalies\n",
    "significant_features: Dict[str, int] = dict()\n",
    "for result in ranked_results:\n",
    "    actor, day, anomaly_score, email, feature_ranks = result\n",
    "    for feature in feature_ranks:\n",
    "        col, percentile_rank, value = feature\n",
    "        if percentile_rank >= cutoff_key or (col in ['is_weekend'] and value > 0):\n",
    "            if col in significant_features:\n",
    "                significant_features[col] += 1\n",
    "            else:\n",
    "                significant_features[col] = 1\n",
    "\n",
    "# sort the significant features by the number of times they appear and print\n",
    "significant_features_list = sorted(significant_features.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"Total features: {len(significant_features)}\")\n",
    "for feature in significant_features_list:\n",
    "    print(f\"{feature[0]}: {feature[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
